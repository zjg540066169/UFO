---
title: "Visualization Jingyi"
author: "Jingyi"
date: "11/9/2019"
output:
  html_document
---

```{r}

# knitr will run the chunk but not include the chunk in the final document
try(source("./dependencies.R"), silent = TRUE)

# ensure reproductivity
set.seed(1)

# load library
library(tidyverse)
library(viridis)
library(ggridges)
library(tidytext)
library(wordcloud2)
data("stop_words")
knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_minimal() + theme(legend.position = "bottom"))

```

```{r read data}

ufo_data = read_csv("./data/tidied_data_final.csv")

ufo = ufo_data %>%  
  separate(date, into = c("month","day","year"), sep = "/") %>% 
  mutate(ufo_shape = forcats::fct_reorder(ufo_shape, encounter_length))
```

# Length Time Descriptive Statistics Table and Plot

```{r Length Descriptive Data}

ufo %>% 
  group_by(country) %>% 
  # create columns for count, mean, median, sd, Q1, Q3, IQR for each country
  summarize(count = n(),
            mean = mean(encounter_length),
            median = median(encounter_length),
            sd = sd(encounter_length),
            Q1 = quantile(encounter_length, 0.25),
            Q3 = quantile(encounter_length, 0.75),
            IQR = IQR(encounter_length)) %>% 
  # arrange according to mean for each group
  arrange(desc(mean)) %>% 
  # knit a table
  knitr::kable(digits = 2, caption = "The mean, median, sd, Q1, Q3 and IQR are for the variable lenght time (in seconds). ")

```

Since we only have `r ufo %>% filter(country == "the United Kingdom") %>% nrow()` observations for the United Kingdom and `r ufo %>% filter(country == "Australia") %>% nrow()` observations for Australia, we are going to omit these two countries. 

Meanwhile, we are looking for reader-friendly and meaningful display of density plot. As we can see that the standard deviations of the United States and Canada are way too large for a "readable" display, we are going to omit extreme values. 

```{r Length Density Plot}

# create dataframe for the plot (which got rid of outliers for each country)
filtered_ufo = ufo %>% 
  group_by(country) %>% 
  # add two columns according to its country: Q3, IQR
  mutate(
    Q3 = quantile(encounter_length, 0.75),
    IQR = IQR(encounter_length)
  ) %>% 
  # ungroup
  ungroup() %>% 
  # filter out outliers
  filter(encounter_length < Q3 + 1.5 * IQR)

# make a density plot
filtered_ufo %>%
  # make x-aixs = encounter_length, fill the color by country
  ggplot(aes(x = encounter_length, fill = country)) +
  # set the transparency to 0.4 and set several customerized parameter
  geom_density(alpha = 0.4, adjust = 0.5, color = "cornsilk4") +
  # to make the fill colors a bit prettier
  scale_fill_viridis(option = "plasma", discrete = TRUE) +
  labs(title = "Density Plot of Length Time for Each Country",
       x = "Encounter Length Time")

```

In the plot, we can see that the United States has the most data and spreaded out a lot with some spikes of encounter length of time. But we can also tell that for the United States, a lot of encounter happened within 500 seconds. We can see that for the United States, there seems to be a pattern of sighting time (due to the spikes). Hence, we can conduct a categorical analysis later in the project. Meanwhile, we also noticed that Canada is somehow similar to the United State, yet "milder". They both share similar spikes and distribution. Hence, we can also try to find if there is a particular pattern behind the UFO sighting, and with certain other data (if we have any), we can maybe explain the why people were seeing UFO's with this pattern.  

# Data Description

In this dataset, it contains time and location data about UFO sighting across `r length(unique(pull(ufo, country)))` countries, from the year of `r min(pull(ufo, year))` to the year of `r max(pull(ufo, year))`. They are ` r nrow(ufo)` observations and `r ncol(ufo)` variables in the dataset. 

The variables in the dataset are:

* `year`: the year of UFO sighting
* `month`: the month of UFO sighting
* `day`: the day of UFO sighting
* `time`: the extra time of UFO sighting
* `city_description`: the extra description that came along with the city of UFO sighting
* `city`: the city if UFO sighting
* `state`: the state of UFO sighting
* `ufo_shape`: the shape of the sighted UFO
* `encounter_length`: the encounter time length of sighting UFO
* `described_encounter_length`: the description from the people who reported the UFO encounter
* `description`: the description of the UFO from the people who reported the UFO encounter
* `date_documented`: the date of documentation of the UFO encounter
* `latitude`: the latitude of UFO encounter
* `longtitude`: the longtitude of UFO encounter
* `country`: the country of UFO sighting

# Lag in Documentation

In the dataset, we noticed that for a lot of the data, there are lags in time regards to their sighting time and the documentation time. Therefore, here is a plot that shows the distribution of lag time in years for each sighting by country.

```{r}

# tidy data
lag_data = ufo %>% 
  # separate the date_documented into month, day and year
  separate(date_documented, into = c("doc_month","doc_day","doc_year"), sep = "/") %>% 
  # add a variable called lag
  mutate(lag = as.numeric(doc_year) - as.numeric(year))
  
# violin plot for the lag
lag_data %>% 
  ggplot(aes(x = country, y = lag)) +
  geom_violin(aes(fill = country), color = "cornsilk4", alpha = 0.4) +
  stat_summary(fun.y = median, geom = "point", color = "darksalmon", size = 3) +
  labs(title = "Violin Plot for the Lag Time in Actual Sighting Date and Documentation Date")

```

This plot indicates that most of the data are documented relatively recent to their sighting, which is a good thing, because the longer the lag, the larger the possibility of wrongly documentation.

## Length Time and Shape Cross Checking

```{r}

# tidy data
length_shape_data = ufo_data %>% 
  mutate(ufo_shape = factor(ufo_shape),
         ufo_shape = forcats::fct_reorder(ufo_shape, encounter_length))

# violin plot 
length_shape_data %>% 
  ggplot(aes(x = ufo_shape, y = encounter_length, group = ufo_shape)) +
  geom_violin(aes(fill = ufo_shape), color = "cornsilk4", alpha = 0.4) +
  theme(axis.text.x = element_text(angle = 300)) +
  stat_summary(fun.y = median, geom = "point", color = "darksalmon", size = 3) +
  labs(title = "The Violin Plot of Encounter Length Time for Each Shape of UFO", 
       y = "Encounter Length of Time",
       x = "UFO Shape")

# function for removing outliers
#remove_outliers <- function(x, shape, na.rm = TRUE, ...) {
#  x = filter(x, ufo_shape == shape) %>% pull(encounter_length)
#  print(x)
#  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
#  H <- 1.5 * IQR(x, na.rm = na.rm)
#  y <- x
#  print(y)
#  y[x < (qnt[1] - H)] <- NA
#  y[x > (qnt[2] + H)] <- NA
#  y
#  }

#length_shape_data_no_outliers = length_shape_data %>% 
#  select(encounter_length, ufo_shape) %>% 
#  map_df(.x = unique(pull(ufo_shape)), ~remove_outliers(., .x, na.rm = FALSE))
  #remove_outliers(.,  unique(), na.rm = FALSE)

#length_shape_data_no_outliers



# violin plot without data for shape with outlier values
#length_shape_data %>% 
#  filter(encounter_length) %>% 
#  ggplot(aes(x = ufo_shape, y = encounter_length, group = ufo_shape)) +
#  geom_violin(aes(fill = ufo_shape), color = "cornsilk4", alpha = 0.4) +
#  theme(axis.text.x = element_text(angle = 300)) +
#  stat_summary(fun.y = median, geom = "point", color = "darksalmon", size = 3) +
#  labs(title = "The Violin Plot of Encounter Length Time for Each Shape of UFO (without outlier)", 
#       y = "Encounter Length of Time",
#       x = "UFO Shape")

# tidy data
median_length_shape_data = ufo_data %>% 
  group_by(ufo_shape) %>% 
  summarize(median_length = median(encounter_length), 
            count = n()) %>% 
  filter(count > 100) %>% 
  arrange(desc(median_length)) %>% 
  ungroup()

median_length_shape_data %>% 
  ggplot(aes(x = ufo_shape, y = median_length, fill = ufo_shape)) + 
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 300)) +
  labs(title = "The Bar Chart of Median of Encounter Length Time for Each Shape of UFO",
       y = "Median of Encounter Length Time",
       x = "UFO Shape")

```


## Word Cloud

```{r}
shape_words = ufo %>% 
  unnest_tokens(word, description) %>% 
  anti_join(stop_words) %>% 
  mutate(word = recode(word, "44" = "ufo")) %>% 
  select(ufo_shape, word) %>% 
  count(ufo_shape, word, sort = TRUE)
  
total_word = shape_words %>% 
  group_by(ufo_shape) %>% 
  summarise(total = sum(n))
  
shape_words = shape_words %>% 
  #left_join(.ï¼Œtotal_word, by = "ufo_shape") %>% 
  #mutate(tfidf = n) %>% 
  group_by(word) %>% 
  mutate(n = sum(n)) %>% 
  #arrange(desc(tfidf)) %>% 
  select(word, n) %>% 
  distinct() %>%
  wordcloud2(shape_words, size = 1,shape = 'star',color = "random-light")

shape_words

```

