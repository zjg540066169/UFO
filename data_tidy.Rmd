---
title: "Tidy Data"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_float: true
---

```{r set up, include = FALSE}
# knitr will run the chunk but not include the chunk in the final document
# copy from Jeff

try(source("./dependencies.R"), silent = TRUE)

# ensure reproductivity
set.seed(10)

# load library
library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)
library(rvest)
library(mapsapi)
library(sf)


knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_bw() + theme(legend.position = "bottom"))
```


In the process of tidy data, we read the original data from github first. The first step we took is to detect any missing values, and we found that there are thousands of missing values in some key variables (state, country and UFO shape). Therefore, we took several counter measures. We overwrite missing values in UFO shape by "unknown" shape, which originally exist in the data as well. We also noticed that some information about country and state were "hidden"/embedded in the city variable, city_area variable seems very untidy. In order to get a clean dataset, we combined information of city_area, country and state to form a list of strings, and by these information, we adopted google API to correct the data and obtain a structured text. Unfortunately, google API did not work perfectly, but the majority of them are good enough for our analysis purpose, we omitted several confusing data but kept most of our tidied data.


## reading data

```{r download, message = FALSE}
ufo_original_data = readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv")

ufo_original_data
```

## Missing Value Detect

```{r detect_missing}
ufo_original_data %>% 
  map_df(.x = ., ~sum(is.na(.x))) %>% 
  knitr::kable()
```

We find the variable "state", "country", "ufo_shape" has largest number of missing values.


## Fill the missing value

According to the relationships of "city_area", "state" and "country", we can fill the missing value.

Since there are some abnormal data in "city_area" like these, we first clean the missing value:

```{r city_abnormal, echo = FALSE}
ufo_original_data %>% 
  select(city_area, state) %>% 
  filter(str_detect(pull(., city_area), "\\("))
```

Since there are a lot of massive data in "city_area", "state" and "country",  we need to tidy the data.

First, to clearly describle the place of insterests, we seperate the variable "city_area" into 2 variable "city_area" and "city_description".

To further explore in place of interests, we write a function to use Google Map`s api, to clean the data.

```{r clean_place, echo = FALSE}

place_tidy = function(city_area, state, country) {
  print(city_area)
  #city_area = df["city_area"]
  #state = df["state"]
  #country = df["country"]
  
  if (is.na(state)) {
    state = ""#"mutate(state, state = "")""
  } else if (is.na(country)) {
    country = ""# mutate(country, country = "")
  }
  place_string = paste(city_area, state, country, sep = ", ")
  
  doc = mp_geocode(addresses = place_string, key = api_key)
  #print(doc)
  pnt = mp_get_points(doc)
  print(pnt)
  pnt
}


#api_key = "AIzaSyC2BBNoODEb5Pg7wbY-Pqvl9VdYRkJ9sHM"


time_step = 100

ufo_tidy_data = 
  ufo_original_data %>% 
  separate(city_area, into = c("city_area", "city_description"), sep = "[\\(\\)/]") %>% 
  #slice((i * time_step + 1):min((i + 1) * time_step), nrow(ufo_original_data)) %>% 
  mutate(state = replace(state, is.na(state), ""), country = replace(country, is.na(country), ""), city_description = replace(city_description, is.na(city_description), "")) %>% 
  mutate(place_string = paste(city_area, city_description, state, country, sep = ", "))
  
```

The code to use Google Map`s api and get the geographic data. Since we do not need to get data again, we comment it. 

```{r}
#df_pnt = 
#  mp_geocode(addresses = pull(ufo_tidy_data, place_string), key = api_key) %>% 
#  mp_get_points() %>% 
#  tibble(
#    .$address,
#    .$address_google,
#    .$pnt,
#    .$status
#  ) %>% 
#  rename("place_string" = ".$address", "address_google" = ".$address_google", "pnt" = ".$pnt", "status" = ".$status") %>% 
#  select(place_string, address_google, pnt, status) %>%
#  mutate(lat = unlist(map(pull(., pnt), 1)),
#           long = unlist(map(pull(., pnt), 2))) %>% 
#  select(-pnt) %>% 
#  write.table(file = paste("./data_pot/", "df.csv", sep = ""), sep = ",", row.names = FALSE)


```


After download all data of the place of interests, we combine all the data in totally more than 800 files.


```{r clean_city, message = FALSE}
ufo_poi = 
  list.files("./data_pot", pattern = "^.+df.csv$") %>%
  map_chr(.x = ., ~paste("./data_pot", ., sep = "/")) %>% 
  map_df(.x = ., read_csv) %>% 
  distinct()
```

After downloading the data from Google Map`s api, we find there is still little mass data, so we tidy them by hand.

```{r merge_poi_data, warning = FALSE}
ufo_tidy =
  ufo_tidy_data %>% 
  full_join(ufo_poi, ., by = c("place_string")) %>% 
  filter(status == "OK") %>% 
  distinct() %>% 
  select(-city_area, -state, -country, -latitude, -longitude,  -status, -place_string) %>% 
  mutate(
    country = str_extract(address_google, "[^,]*$"),
    state = str_remove(address_google, "[^,]*$"),
    country = str_remove(country, "^[\\s0-9]*"),
    country = str_remove(country, "[\\s0-9]*$"),
    city = str_remove(state, "[^,]*,$"),
    city = str_remove(city, "[\\s]*,$"),
    state = str_extract(state, "[^,]*,$"),
    state = str_remove(state, "^[\\s0-9]*"),
    state = str_remove(state, "[\\s0-9]*,$")
    ) #%>%

australia = 
  ufo_tidy %>% 
  filter(country == "Australia") %>% 
  separate(state, sep = "\\s", into = c("city", "state"))

ufo_tidy =
  ufo_tidy %>% 
  left_join(australia, by = colnames(australia)[1:11]) %>%
  mutate(
    state = if_else(country == "Australia", state.y, state.x),
    city =  if_else(country == "Australia", city.y, city.x),
         ) %>% 
  select(-state.y, -city.y, -state.x, -city.x) %>% 
  mutate(
    city = replace(city, city == "" || is.na(city), "unknown"),
    state = replace(state, state == "" || is.na(state), "unknown"),
    country = replace(country, country == "" || is.na(country), "unknown"),
    country = replace(country, country == "United States", "USA"),
    country = replace(country, country == "United Kingdom", "UK"),
    temp = lat,
    lat = long,
    long = temp
  ) %>% 
  mutate(state = recode(state , 
                "Alabama" = "AL", "Oregon" = "OR","Ohio" = "OH","New York" = "NY","New Mexico" = "NM", 
                "Alaska" = "AK", "Nevada" = "NV","Nebraska" = "NE", "Florida" = "FL","Georgia" = "GA",
                "Arizona" = "AZ","Idaho" = "ID","Illinois" = "IL","Indiana" = "IN","Iowa" = "IA",
                "Kansas" = "KS","Missouri" = "MO","Kentucky" = "KY","Louisiana" = "LA","Maine" = "ME",
                "Maryland" = "MD","Massachusetts" = "MA","Michigan" = "MI","Minnesota" = "MN",
                 "California" = "CA","Texas" = "TX","Tennessee" = "TN","Pennsylvania" = "PA",
                "Colorado" = "CO","Washington" = "WA","Vermont" = "VT","Utah" = "UT"),
         ufo_shape = recode(ufo_shape, "changed" = "changing")
         ) %>% 
  select(-address_google, -temp) %>% 
  rename("latitude" = "lat", "longitude" = "long")
```

Here, we assign "unknown" to mssing value in ufo_shape, 

```{r shape_unknown, warning = FALSE}
ufo_tidy = 
  ufo_tidy %>% 
  mutate(ufo_shape = replace(ufo_shape, is.na(ufo_shape), "unknown")) 
```

Delete all the rows with missing value and save the cleaned data. Since we do not need to save the tidied data again, we comment the code here.

```{r NA, warning = FALSE}

ufo_tidy %>% 
  drop_na() %>%
  filter(Encoding(city) != "UTF-8") %>%
  
  separate(date_time, into = c("date", "time"), sep = " ")# %>% 
  #write.table(file = paste("./", "tidied_data_final.csv", sep = ""), sep = ",", row.names = FALSE)
```