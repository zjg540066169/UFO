---
title: "data_tidy"
author: "Jungang Zou"
date: "11/9/2019"
output: github_document
---

```{r set up, include = FALSE}
# knitr will run the chunk but not include the chunk in the final document
# copy from Jeff

try(source("./dependencies.R"), silent = TRUE)

# ensure reproductivity
set.seed(10)

# load library
library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)
library(rvest)
library(mapsapi)
library(sf)


knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## reading data

```{r download}
ufo_original_data = readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv")

ufo_original_data
```

## Missing Value Detect

```{r detect_missing}
ufo_original_data %>% 
  map_df(.x = ., ~sum(is.na(.x))) %>% 
  knitr::kable()
```

We find the variable "state", "country", "ufo_shape" has largest number of missing values.


## Fill the missing value

According to the relationships of "city_area", "state" and "country", we can fill the missing value.

Since there are some abnormal data in "city_area" like these, we first clean the missing value:

```{r city_abnormal, echo = FALSE}
ufo_original_data %>% 
  select(city_area, state) %>% 
  filter(str_detect(pull(., city_area), "\\("))
```

Since there are a lot of massive data in "city_area", "state" and "country",  we need to tidy the data.

First, to clearly describle the place of insterests, we seperate the variable "city_area" into 2 variable "city_area" and "city_description".

To further explore in place of interests, we write a function to use Google Map`s api, to clean the data.

```{r clean_place, echo = FALSE}

place_tidy = function(city_area, state, country) {
  print(city_area)
  #city_area = df["city_area"]
  #state = df["state"]
  #country = df["country"]
  
  if (is.na(state)) {
    state = ""#"mutate(state, state = "")""
  } else if (is.na(country)) {
    country = ""# mutate(country, country = "")
  }
  place_string = paste(city_area, state, country, sep = ", ")
  
  doc = mp_geocode(addresses = place_string, key = api_key)
  #print(doc)
  pnt = mp_get_points(doc)
  print(pnt)
  pnt
}


api_key = "AIzaSyC2BBNoODEb5Pg7wbY-Pqvl9VdYRkJ9sHM"


time_step = 100

ufo_tidy_data = 
  ufo_original_data %>% 
  separate(city_area, into = c("city_area", "city_description"), sep = "[\\(\\)/]") %>% 
  #slice((i * time_step + 1):min((i + 1) * time_step), nrow(ufo_original_data)) %>% 
  mutate(state = replace(state, is.na(state), ""), country = replace(country, is.na(country), ""), city_description = replace(city_description, is.na(city_description), "")) %>% 
  mutate(place_string = paste(city_area, city_description, state, country, sep = ", "))
  
  
  
df_pnt = 
  mp_geocode(addresses = pull(ufo_tidy_data, place_string), key = api_key) %>% 
  mp_get_points() %>% 
  tibble(
    .$address,
    .$address_google,
    .$pnt,
    .$status
  ) %>% 
  rename("place_string" = ".$address", "address_google" = ".$address_google", "pnt" = ".$pnt", "status" = ".$status") %>% 
  select(place_string, address_google, pnt, status) %>%
  mutate(lat = unlist(map(pull(., pnt), 1)),
           long = unlist(map(pull(., pnt), 2))) %>% 
  select(-pnt) %>% 
  write.table(file = paste("./data_pot/", "df.csv", sep = ""), sep = ",", row.names = FALSE)

  #map_df(.x = pull(., pnt), ~unclass(.x))
#mutate(df_pnt, unnest(pnt))
  #
  
#ufo_tidy_data = pmap_df(list(ufo_tidy_data$city_area, ufo_tidy_data$state, ufo_tidy_data$country), place_tidy)
  #pmap(list(pull(., city_area), pull(., state), pull(., country)), ~place_tidy)

```

After download all data of the place of interests, we combine all the data in totally more than 800 files.


```{r clean_city, message = FALSE}
ufo_poi = 
  list.files("./data_pot", pattern = "^.+df.csv$") %>%
  map_chr(.x = ., ~paste("./data_pot", ., sep = "/")) %>% 
  map_df(.x = ., read_csv)
```

```{r merge_poi_data, warning = FALSE}
ufo_tidy =
  ufo_tidy_data %>% 
  full_join(ufo_poi, ., by = c("place_string")) %>% 
  filter(status == "OK") %>% 
  select(-city_area, -state, -country, -latitude, -longitude, -place_string, -status) %>% 
  separate(address_google, sep = ",", into = c("city", "state", "country")) %>% 
  separate(state, sep = ". ", into = c("state", "zip_code")) %>% 
  select(-zip_code) %>% 
  distinct() %>% 
  rename("latitude" = "lat", latitude = "long")
```

We assign "unknown" to NA in ufo_shape, 

```{r shape_unknown, warning = FALSE}
ufo_tidy = 
  ufo_tidy %>% 
  mutate(ufo_shape = replace(ufo_shape, is.na(ufo_shape), "unknown")) 
```

delete all the rows with NA

```{r NA, warning = FALSE}
ufo_tidy = 
  ufo_tidy %>% 
  drop_na() %>% 
  write.table(file = paste("./", "tidied_data_final.csv", sep = ""), sep = ",", row.names = FALSE)
```