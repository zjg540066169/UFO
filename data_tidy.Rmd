---
title: "data_tidy"
author: "Jungang Zou"
date: "11/9/2019"
output: github_document
---

```{r set up, include = FALSE}
# knitr will run the chunk but not include the chunk in the final document
# copy from Jeff

try(source("./dependencies.R"), silent = TRUE)

# ensure reproductivity
set.seed(10)

# load library
library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)
library(rvest)
library(mapsapi)


knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## reading data

```{r download}
ufo_original_data = readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv")

ufo_original_data
```

## Missing Value Detect

```{r detect_missing}
ufo_original_data %>% 
  map_df(.x = ., ~sum(is.na(.x))) %>% 
  knitr::kable()
```

We find the variable "state", "country", "ufo_shape" has largest number of missing values.


## Fill the missing value

According to the relationships of "city_area", "state" and "country", we can fill the missing value.

Since there are some abnormal data in "city_area" like these, we first clean the missing value:

```{r city_abnormal, echo = FALSE}
ufo_original_data %>% 
  select(city_area, state) %>% 
  filter(str_detect(pull(., city_area), "\\("))
```

Since there are a lot of massive data in "city_area", "state" and "country", we write a function to use Google Map`s api, to clean the data.

```{r clean_place, echo = FALSE}
api_key = "AIzaSyAxm_kcnsTq0QRM6Ovnz3C_KqAsEF6XqgY"
place_tidy = function(df) {
  print(df)
  city_area = df["city_area"]
  state = df["state"]
  country = df["country"]
  
  if (is.na(state)) {
    state = mutate(state, state = "")
  } else if (is.na(country)) {
    country = mutate(country, country = "")
  }
  place_string = paste(city_area, state, country, sep = ",")
  doc = mp_geocode(addresses = place_string)
  pnt = mp_get_points(doc)
  pnt
}


ufo_tidy_data = 
  ufo_original_data %>% 
  #separate(city_area, into = c("city_area", "city_description"), sep = "[\\(\\)/]") %>% 
  slice(1:10) %>% 
  #pull(city_area)
  #print(ufo_tidy_data[c("city_area", "state", "country")])
  select(city_area, state, country) %>% 
  
  map(.x = ., ~place_tidy(city_area))

```


We seperate the variable "city_area" into 2 variable "city_area" and "city_description"

```{r clean_city, echo = FALSE}
ufo_tidy_data = 
  ufo_original_data %>% 
  separate(city_area, into = c("city_area", "city_description"), sep = "[\\(\\)/]")# %>% 
print(unique(pull(ufo_original_data, city_area)))
```