---
title: "UFO Prediction"
output:
   html_document:
    code_folding: show
    toc: true
    toc_float: true
---


```{r set up, include = FALSE}
# knitr will run the chunk but not include the chunk in the final document
# copy from Jeff
try(source("./dependencies.R"), silent = TRUE)

# ensure reproductivity
set.seed(10)

# load library

library(viridis)
library(ggridges)
library(patchwork)
library(rvest)
library(MLmetrics)
library(e1071)
library(tidyverse)
library(modelr)


knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  message = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_bw() + theme(legend.position = "bottom"))
```


# *Outlier Detection for UFO Prediction*

By the analysis we did before, we found that association does exist in UFO appearance time and location. To enhance prediction accuracy, we adapted the powerful tool of Machine Learning in assistance of our prediction. Unfortunately, we only have the "positive" samples in the dataset, which means only the UFO appearance data are availiable to us and the data about UFO not appearing are missing. Under this situation, we view prediction of whether UFO is appearing or not as a problem of one class classification, aka outlier detection. Here we take advantage of a popular method that combats this adversity, One Class Support Vector Machine. In this process. we fabricated "negative" samples that would be included in test data of the cross validation procedure. 


```{r message = FALSE, warning = FALSE}
# read data
ufo_data = read_csv(file = "./data/tidied_data_final.csv")



# data tidy for prediction
ufo =
  ufo_data %>%  
  filter(country == "USA") %>% 
  separate(date, into = c("month","day","year"), sep = "/") %>% 
  separate(time, into = c("hour","minute"), sep = ":") %>% 
  select(year, month, day, hour, minute,state, city) %>%
  na.omit(ufo_data) %>% 
  mutate(add = paste(state, city)) %>% 
  mutate(month = as.factor(month.name[as.numeric(month)]), year = as.numeric(year), hour = as.numeric(hour), minute = as.numeric(minute), day = as.numeric(day))
```
  
  
```{r}  

# keep the city that the ufo appears with more than 100 times, and delete others
ufo_city = 
  ufo %>% 
  dplyr::group_by(add) %>% 
  mutate(count = dplyr::n()) %>% 
  filter(count >= 100) %>% 
  ungroup() %>% 
  select(-count) %>% 
  select(-state, -city) %>% 
  mutate(add = as.factor(add))

```


Since we are predicting by city, which is a categorical data, each city will be treated as a dummy variable in the SVM model. Hence, we limited the city terms down to the cities that have more than 100 times of UFO sightings. 

Usually, SVM model will provide analysts kernel methods to choose. RBF kernel and linear kernel are the most frequently used methods. Given that RBF is usually effective and good for non-linear data, the training of RBF are time-consuming and the hyperparameters are needed to be tuned for achieveing the best result. We here used an easier-to-train and only one parameter (nu), in contrast with RBF which contains more than two parameters. To elaborate on the parameter nu is that the statistical meaning of this parameter is an upper bound on the fraction of outliers. Because the fraction in population is unknown, we need to estimate the hyperparameter by cross validation.


# *Cross Validation Preparation*

```{r}

# set up the result of cross validation table
nu_cv = tibble(
  nu = numeric(),
  f1_svm = list()
)

# transform each levels in factors to new variables to form the input of SVM
factor_variable_cope = function(df){
  if (is.tibble(df)) {
    df = as.data.frame(df)
  }
  for (i in colnames(df[, sapply(df, is.factor)])) {
    for (level in unique(df[, i])) {
        df[paste(i, level, sep = "_")] = 
            as.integer(ifelse(df[, i] == level, 1, -1))
    }
  }
  df
}

```


# Cross Validation to tune the "nu" parameters for One Class Support Vector Machine

We here used tenfold cross validation and the sample size of the fabricated data is 1/10 of the sample size of the positive dataset. Hence, during the cross validation, sample size of the test set and training set are the same.

Use F1_score to evaluate the prediction result. F1_score can use to balance the result of unbalanced data.

Here is the link to some background about F1 score:
https://en.wikipedia.org/wiki/F1_score


```{r}

# tune the parameter for nu in OC-SVM
for (nu_ in c(0.001, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99)) {
  true_n = nrow(ufo_city)  #numbers of true samples
  false_samples = true_n / 10 # numbers of false samples

  false_data = tibble( # false samples randomly generating
    year = sample(1900:2100, false_samples, replace = T),
    month = sample(ufo_city$month, false_samples, replace = T),
    day = sample(1:30, false_samples, replace = T),
    hour = sample(0:24, false_samples, replace = T),
    minute = sample(ufo_city$minute, false_samples, replace = T),
    add = sample(ufo_city$add, false_samples, replace = T)
  ) %>% 
    anti_join(ufo_city, by = c("year", "month", "day", "hour", "minute", "add")) # delete the samples of true in false dataset
  
  
  false_n = nrow(false_data) #numbers of false samples
  
  
  
  # here we bind rows of true and false dataset to deal with the factor variables
  train_df = 
    ufo_city %>% 
    bind_rows(false_data) %>% 
    factor_variable_cope() %>% 
    select(-month, -add) %>% 
    mutate(year = as.integer(year), day = as.integer(day), hour = as.integer(hour), minute = as.integer(minute)) %>% 
    sample_frac(1)
  
  
  # seperate the false and true samples from transformed data
  false_df = 
    train_df %>% 
    slice((true_n + 1):(true_n + false_n)) %>% 
    mutate(y = FALSE) %>% 
    as_tibble() 
  
  true_df = 
    train_df %>% 
    slice(1:true_n) %>% 
    #mutate(y = TRUE) %>% 
    as_tibble() 
  
  
  # construct the cross validation dataset from true samples datasets
  cv_df = 
    crossv_mc(true_df, 10) %>% 
    mutate(
      train = map(train, as_tibble),
      test = map(test, as_tibble)
      )
  
  # add false samples to test dataset and construct the true labels 
  for (i in 1:nrow(cv_df)) {
    cv_df$test[[i]] = mutate(cv_df$test[[i]], y = TRUE)
    cv_df$test[[i]] = rbind(cv_df$test[[i]], false_df)
    cv_df$label[[i]] = cv_df$test[[i]]$y
    cv_df$test[[i]] = select(cv_df$test[[i]], -y)
  }
   
  # use OC-SVM to predict
  cv_model = 
    cv_df %>% 
    mutate(svm_mod = map(train, ~svm(.x , y = NULL,
             type = 'one-classification',
             nu = nu_, # tune the parameters by cross validation
             scale = TRUE, 
             kernel = "linear"))) %>% 
    mutate(pred_label = map2(svm_mod, test, ~predict(.x, .y)))
  
  # calculate the f1-score for each prediction
  cv_f1 =
    cv_model %>% 
    mutate(f1_svm = map2_dbl(.x = label, .y = pred_label, ~F1_Score(y_true = .x, y_pred = .y))) 
  
  # save the result 
  nu_cv = 
    tibble(
      nu = nu_,
      f1_svm = mean(pull(cv_f1, f1_svm))
    ) %>% 
    rbind(nu_cv, .)
  
}
```

One Class Cross Valiadation is not an exact copy from what we have done in class, but the specific method we used here were included and explained more in-depth in the paper cited in the end of this page.


# *Draw the graph of f1_score for each nu*

```{r}
# change the scale of data for graph
cvp = 
  nu_cv %>% 
  mutate(nu = case_when(
    nu <= 0.3 ~ log10(nu),
    nu > 0.3 ~ exp(nu)
  ))

# draw the graph
cvp %>% 
  ggplot(aes(x = nu, y = f1_svm, color = nu)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(
    breaks = pull(cvp, nu),
    labels = c("0.001", "0.005", "0.01", "0.05", "0.1", "0.3", "0.5", "0.7", "0.9", "0.99")
  ) +
  labs(
    title = "Average F1 Score for nu in SVM by 10 folds CV",
    x = "nu",
    y = "Average F1 Score") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```

According the graph, we find nu = 0.5 is best parameter for OC-SVM.

After geting the best nu, we use it to train OC-SVM again.


# *Train the Final Best Model with Best nu = 0.5*

```{r}
nu_ = 0.5

# train svm models
svm_model = 
    ufo_city %>% 
    factor_variable_cope() %>% 
    select(-month, -add) %>% 
    mutate(year = as.integer(year), day = as.integer(day), hour = as.integer(hour), minute = as.integer(minute)) %>% 
    sample_frac(1) %>% 
    svm(y = NULL,
        type = 'one-classification',
        nu = nu_,
        scale = TRUE, 
        kernel = "linear"
        )
  
# display the models
svm_model


saveRDS(svm_model, "SVM.rds")

# transform each levels in factors to new variables to form the input of SVM
factor_variable_cope = function(df){
    if (is_tibble(df)) {
      df = as.data.frame(df)
    }
    for (i in colnames(df[, sapply(df, is.factor)])) {
      for (level in unique(df[, i])) {
          df[paste(i, level, sep = "_")] = 
              as.integer(ifelse(df[, i] == level, 1, -1))
      }
    }
    df
}


sample = tibble(
  year = 1900,
  month =  "October",
  day = 10,
  hour = 23,
  minute = 0,
  add = "CA San Bernardino"
)



svm_model = readRDS("SVM.rds")
  
  



  svm = 
    ufo %>% 
    select(year, month, day, hour, minute,state, city) %>%
    na.omit(ufo_data) %>% 
    mutate(add = paste(state, city)) %>% 
    mutate(year = as.numeric(year), month = month.name[as.numeric(month)], hour = as.numeric(hour), minute = as.numeric(minute), day = as.numeric(day)) %>% 
    select(-city, -state) %>% 
    bind_rows(sample, .) %>% 
    mutate(month = as.factor(month)) %>% 
    dplyr::group_by(add) %>% 
    mutate(count = dplyr::n()) %>% 
    filter(count >= 100) %>% 
    ungroup() %>% 
    select(-count) %>% 
    mutate(add = as.factor(add)) %>% 
    as.data.frame() %>% 
    factor_variable_cope() %>% 
    slice(1) %>% 
    select(-month, -add) %>% 
    mutate(year = as.integer(year), day = as.integer(day), hour = as.integer(hour), minute = as.integer(minute)) %>%
    predict(svm_model, .)


```



# *Work Cited*

Manevitz, L. M., & Yousef, M. (2001). One-class SVMs for document classification. Journal of machine Learning research, 2(Dec), 139-154.

Hempstalk, K., & Frank, E. (2008, December). Discriminating against new classes: One-class versus multi-class classification. In Australasian Joint Conference on Artificial Intelligence (pp. 325-336). Springer, Berlin, Heidelberg.
